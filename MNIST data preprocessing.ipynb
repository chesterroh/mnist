{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and Verified notMNIST_large.tar.gz\n",
      "Found and Verified notMNIST_small.tar.gz\n",
      "notMNIST_large already present - skipping extraction of notMNIST_large.tar.gz.\n",
      "['notMNIST_large/A', 'notMNIST_large/B', 'notMNIST_large/C', 'notMNIST_large/D', 'notMNIST_large/E', 'notMNIST_large/F', 'notMNIST_large/G', 'notMNIST_large/H', 'notMNIST_large/I', 'notMNIST_large/J']\n",
      "notMNIST_small already present - skipping extraction of notMNIST_small.tar.gz.\n",
      "['notMNIST_small/A', 'notMNIST_small/B', 'notMNIST_small/C', 'notMNIST_small/D', 'notMNIST_small/E', 'notMNIST_small/F', 'notMNIST_small/G', 'notMNIST_small/H', 'notMNIST_small/I', 'notMNIST_small/J']\n",
      "notMNIST_large/A\n",
      "notMNIST_large/A/U3RhcmJ1cnN0IEl0YWxpYy50dGY=.png\n",
      "notMNIST_large/A/TmFkaW5lIFRoaW4gQm9sZC50dGY=.png\n",
      "notMNIST_large/B\n",
      "notMNIST_large/B/SW50ZXJzdGF0ZS1CbGFjay5vdGY=.png\n",
      "notMNIST_large/B/SmFuYWtpLUJvbGRJdGFsaWMub3Rm.png\n",
      "notMNIST_large/C\n",
      "notMNIST_large/C/VHJ1bXBNZWRpYWV2YWwtQm9sZC5vdGY=.png\n",
      "notMNIST_large/C/RnJlZWRvbSA5IFRoaW4gTm9ybWFsLnR0Zg==.png\n",
      "notMNIST_large/D\n",
      "notMNIST_large/D/UGhvdGluYU1UU3RkLVVsdHJhQm9sZC5vdGY=.png\n",
      "notMNIST_large/D/SW50dWl0aW9uIFNTaSBCb2xkLnR0Zg==.png\n",
      "notMNIST_large/E\n",
      "notMNIST_large/E/SVRDIFF1b3J1bSBNZWRpdW0ucGZi.png\n",
      "notMNIST_large/E/Q2VudG8gRXh0ZW5kZWQgQm9sZC50dGY=.png\n",
      "notMNIST_large/F\n",
      "notMNIST_large/F/TGltZUdsb3J5Q2Fwcy50dGY=.png\n",
      "notMNIST_large/F/QmFsdHJhLnBmYg==.png\n",
      "notMNIST_large/G\n",
      "notMNIST_large/G/T2xkLVRvd25FeHQtTm9ybWFsLnR0Zg==.png\n",
      "notMNIST_large/G/Um9zZSBSb3VuZCBCb2xkLnR0Zg==.png\n",
      "notMNIST_large/H\n",
      "notMNIST_large/H/TWlkZXZpbCBOb3JtYWwudHRm.png\n",
      "notMNIST_large/H/SmFlZ2VyQW50aXF1YUJRLUJvbGQub3Rm.png\n",
      "notMNIST_large/I\n",
      "notMNIST_large/I/UmFiYml0RWFycy5vdGY=.png\n",
      "notMNIST_large/I/WmFnLnR0Zg==.png\n",
      "notMNIST_large/J\n",
      "notMNIST_large/J/Q2FydHdyaWdodCBSZWd1bGFyLnR0Zg==.png\n",
      "notMNIST_large/J/R1VOQkFUUy50dGY=.png\n",
      "notMNIST_large/A.pickle already exists - skipping\n",
      "notMNIST_large/B.pickle already exists - skipping\n",
      "notMNIST_large/C.pickle already exists - skipping\n",
      "notMNIST_large/D.pickle already exists - skipping\n",
      "notMNIST_large/E.pickle already exists - skipping\n",
      "notMNIST_large/F.pickle already exists - skipping\n",
      "notMNIST_large/G.pickle already exists - skipping\n",
      "notMNIST_large/H.pickle already exists - skipping\n",
      "notMNIST_large/I.pickle already exists - skipping\n",
      "notMNIST_large/J.pickle already exists - skipping\n",
      "notMNIST_small/A.pickle already exists - skipping\n",
      "notMNIST_small/B.pickle already exists - skipping\n",
      "notMNIST_small/C.pickle already exists - skipping\n",
      "notMNIST_small/D.pickle already exists - skipping\n",
      "notMNIST_small/E.pickle already exists - skipping\n",
      "notMNIST_small/F.pickle already exists - skipping\n",
      "notMNIST_small/G.pickle already exists - skipping\n",
      "notMNIST_small/H.pickle already exists - skipping\n",
      "notMNIST_small/I.pickle already exists - skipping\n",
      "notMNIST_small/J.pickle already exists - skipping\n",
      "Number of images in  notMNIST_large/A : 52909\n",
      "Number of images in  notMNIST_large/B : 52911\n",
      "Number of images in  notMNIST_large/C : 52912\n",
      "Number of images in  notMNIST_large/D : 52911\n",
      "Number of images in  notMNIST_large/E : 52912\n",
      "Number of images in  notMNIST_large/F : 52912\n",
      "Number of images in  notMNIST_large/G : 52912\n",
      "Number of images in  notMNIST_large/H : 52912\n",
      "Number of images in  notMNIST_large/I : 52912\n",
      "Number of images in  notMNIST_large/J : 52911\n",
      "Number of images in  notMNIST_small/A : 1872\n",
      "Number of images in  notMNIST_small/B : 1873\n",
      "Number of images in  notMNIST_small/C : 1873\n",
      "Number of images in  notMNIST_small/D : 1873\n",
      "Number of images in  notMNIST_small/E : 1873\n",
      "Number of images in  notMNIST_small/F : 1872\n",
      "Number of images in  notMNIST_small/G : 1872\n",
      "Number of images in  notMNIST_small/H : 1872\n",
      "Number of images in  notMNIST_small/I : 1872\n",
      "Number of images in  notMNIST_small/J : 1872\n",
      "['notMNIST_large/A.pickle', 'notMNIST_large/B.pickle', 'notMNIST_large/C.pickle', 'notMNIST_large/D.pickle', 'notMNIST_large/E.pickle', 'notMNIST_large/F.pickle', 'notMNIST_large/G.pickle', 'notMNIST_large/H.pickle', 'notMNIST_large/I.pickle', 'notMNIST_large/J.pickle']\n",
      "['notMNIST_small/A.pickle', 'notMNIST_small/B.pickle', 'notMNIST_small/C.pickle', 'notMNIST_small/D.pickle', 'notMNIST_small/E.pickle', 'notMNIST_small/F.pickle', 'notMNIST_small/G.pickle', 'notMNIST_small/H.pickle', 'notMNIST_small/I.pickle', 'notMNIST_small/J.pickle']\n",
      "Training:  (480000, 28, 28) (480000,)\n",
      "Validation:  (20000, 28, 28) (20000,)\n",
      "Testing:  (18000, 28, 28) (18000,)\n",
      "compress size 3250968521\n",
      "Overlapping image from test removed : 4434\n",
      "Overlapping image from valid removed :  4066\n",
      "Compressed pickle sanit size: 3197622503\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from scipy import ndimage\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "\n",
    "def maybe_download(filename,expected_bytes, force=False):\n",
    "    if force or not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url+filename,filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and Verified',filename)\n",
    "    else:\n",
    "        raise Exception(\n",
    "            'Failed to verify' + filename + 'please get it with browser')\n",
    "    return filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\n",
    "\n",
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename,force=False):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]\n",
    "    if os.path.isdir(root) and not force:\n",
    "        print('%s already present - skipping extraction of %s.' % ( root, filename ))\n",
    "    else:\n",
    "        print('Extracing data for %s. This may take a while.' % root )\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "    data_folders = [\n",
    "        os.path.join(root,d) for d in sorted(os.listdir(root))\n",
    "        if os.path.isdir(os.path.join(root,d))]\n",
    "    if len(data_folders) != num_classes:\n",
    "        raise Exception('Expected %d folders, found %d instead' % (num_classes,len(data_folders)))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "\n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)\n",
    "\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "def disp_samples(data_folders,sample_size):\n",
    "    for folder in data_folders:\n",
    "        print(folder)\n",
    "        image_files = os.listdir(folder)\n",
    "        image_sample = random.sample(image_files,sample_size)\n",
    "        for image in image_sample:\n",
    "            image_file = os.path.join(folder,image)\n",
    "            print(image_file)\n",
    "            #image = mpimg.imread(image_file)\n",
    "            #plt.imshow(image)\n",
    "            #plt.show()\n",
    "            \n",
    "disp_samples(train_folders,2)\n",
    "#disp_samples(test_folders,10)\n",
    "\n",
    "image_size = 28    # pixel width and height\n",
    "pixel_depth = 255.0 # number of levels per pixel\n",
    "\n",
    "def load_letter(folder,min_num_images):\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files),image_size,image_size),dtype=np.float32)\n",
    "    image_index = 0\n",
    "    print(folder)\n",
    "    for image in os.listdir(folder):\n",
    "        image_file = os.path.join(folder,image)\n",
    "        try:\n",
    "            image_data = ( ndimage.imread(image_file).astype(np.float32) - pixel_depth / 2 ) / pixel_depth\n",
    "            if image_data.shape != (image_size,image_size):\n",
    "                raise Exception('Unexpected image shape : %s' % str(image_data.shape))\n",
    "            dataset[image_index,:,:] = image_data\n",
    "            image_index += 1\n",
    "        except IOError as e:\n",
    "            print('could not read:' , image_file, ':', e, ' it\\'s okay just skipping.. ')\n",
    "    num_images = image_index\n",
    "    final_dataset = dataset[0:num_images,:,:]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Main fewer images than expected %d < %d ' % ( num_images,min_num_images))\n",
    "    print('Full dataset tensor:' , final_dataset.shape)\n",
    "    print('Mean : ', np.mean(final_dataset))\n",
    "    print('Standard deviation:', np.std(final_dataset))\n",
    "    return final_dataset\n",
    "\n",
    "def maybe_pickle(data_folders,min_num_images_per_class,force=False):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            print('%s already exists - skipping' % set_filename)\n",
    "        else:\n",
    "            print('pickling %s..' % set_filename)\n",
    "            dataset = load_letter(folder,min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset,f,pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('unable to save data to', set_filename, ':', e)\n",
    "    return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders,52900)\n",
    "test_datasets = maybe_pickle(test_folders,1870)\n",
    "\n",
    "def disp_number_images(data_folders):\n",
    "    for folder in data_folders:\n",
    "        pickle_filename = ''.join(folder) + '.pickle'\n",
    "        try:\n",
    "            with open(pickle_filename,'rb') as f:\n",
    "                dataset = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print('Unable to read data from', pickle_filename, ':', e)\n",
    "            return\n",
    "        print('Number of images in ', folder, ':', len(dataset))\n",
    "\n",
    "disp_number_images(train_folders)\n",
    "disp_number_images(test_folders)\n",
    "\n",
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows,img_size,img_size), dtype=np.float)\n",
    "        labels = np.ndarray(nb_rows,dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size = 0 ):\n",
    "    num_classes = len(pickle_files)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size,image_size)\n",
    "    train_dataset, train_labels = make_arrays(train_size,image_size)\n",
    "    vsize_per_class = valid_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "\n",
    "    start_v, start_t = 0 , 0\n",
    "    end_v, end_t  = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class + tsize_per_class\n",
    "\n",
    "    for label, pickle_file in enumerate(pickle_files):\n",
    "        try:\n",
    "            with open(pickle_file,'rb') as f:\n",
    "                letter_set = pickle.load(f)\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter = letter_set[:vsize_per_class,:,:]\n",
    "                    valid_dataset[start_v:end_v,:,:] = valid_letter\n",
    "                    valid_labels[start_v:end_v] = label\n",
    "                    start_v += vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "\n",
    "                train_letter = letter_set[vsize_per_class:end_l,:,:]\n",
    "                train_dataset[start_t:end_t,:,:] = train_letter\n",
    "                train_labels[start_t:end_t] = label\n",
    "                start_t += tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "                \n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':',e)\n",
    "            raise\n",
    "\n",
    "    return valid_dataset,valid_labels,train_dataset,train_labels\n",
    "\n",
    "train_size = 480000\n",
    "valid_size = 20000\n",
    "test_size = 18000\n",
    "\n",
    "print(train_datasets)\n",
    "print(test_datasets)\n",
    "\n",
    "valid_dataset,valid_labels, train_dataset, train_labels = merge_datasets(train_datasets,train_size,valid_size)\n",
    "_,_,test_dataset,test_labels = merge_datasets(test_datasets,test_size)\n",
    "\n",
    "print('Training: ', train_dataset.shape,train_labels.shape)\n",
    "print('Validation: ', valid_dataset.shape,valid_labels.shape)\n",
    "print('Testing: ', test_dataset.shape,test_labels.shape)\n",
    "\n",
    "def randomize(dataset,labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "\n",
    "pretty_labels = { 0: 'A', 1: 'B', 2: 'C',3: 'D',4: 'E',5: 'F',6: 'G',7: 'H',8: 'I',9: 'J'}\n",
    "\n",
    "def disp_sample_dataset(dataset,labels):\n",
    "    items = random.sample(range(len(labels)),8)\n",
    "    for i, item in enumerate(items):\n",
    "        plt.subplot(2,4,i+1)\n",
    "        plt.axis('off')\n",
    "        plt.title(pretty_labels[labels[item]])\n",
    "        plt.imshow(dataset[item])\n",
    "\n",
    "disp_sample_dataset(train_dataset,train_labels)\n",
    "disp_sample_dataset(valid_dataset,valid_labels)\n",
    "disp_sample_dataset(test_dataset,test_labels)\n",
    "\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file,'wb')\n",
    "\n",
    "    save = {\n",
    "        'train_dataset' : train_dataset,\n",
    "        'train_labels' : train_labels,\n",
    "        'valid_dataset' : valid_dataset,\n",
    "        'valid_labels' : valid_labels,\n",
    "        'test_dataset' : test_dataset,\n",
    "        'test_labels' : test_labels\n",
    "        }\n",
    "    pickle.dump(save,f,pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print('Unable to save data to',pickle_file,':',e)\n",
    "    raise\n",
    "\n",
    "\n",
    "statinfo = os.stat(pickle_file)\n",
    "print(\"compress size\",statinfo.st_size)\n",
    "\n",
    "def sanetize(dataset_1,dataset_2,labels_1):\n",
    "    dataset_hash_1 = np.array([hashlib.sha256(img).hexdigest() for img in dataset_1])\n",
    "    dataset_hash_2 = np.array([hashlib.sha256(img).hexdigest() for img in dataset_2])\n",
    "    overlap = []\n",
    "    for i, hash1 in enumerate(dataset_hash_1):\n",
    "        duplicates = np.where(dataset_hash_2 == hash1 )\n",
    "        if len(duplicates[0]):\n",
    "            overlap.append(i)\n",
    "    return np.delete(dataset_1,overlap,0),np.delete(labels_1,overlap,None)\n",
    "\n",
    "test_dataset_sanit, test_labels_sanit = sanetize(test_dataset,train_dataset, test_labels)\n",
    "print('Overlapping image from test removed :', len(test_dataset) - len(test_dataset_sanit))\n",
    "valid_dataset_sanit, valid_labels_sanit = sanetize(valid_dataset,train_dataset, valid_labels)\n",
    "print('Overlapping image from valid removed : ', len(valid_dataset) - len(valid_dataset_sanit))\n",
    "\n",
    "pickle_file_sanit = 'notMNIST_sanit.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file_sanit,'wb')\n",
    "    save = {\n",
    "        'train_dataset' : train_dataset,\n",
    "        'train_labels' : train_labels,\n",
    "        'valid_dataset' : valid_dataset_sanit,\n",
    "        'valid_labels' : valid_labels_sanit,\n",
    "        'test_dataset' : test_dataset_sanit,\n",
    "        'test_labels' : test_labels_sanit\n",
    "        }\n",
    "    pickle.dump(save,f,pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print('Unable to save data to',pickle_file_sanit, ':', e)\n",
    "    raise\n",
    "\n",
    "statinfo = os.stat(pickle_file_sanit)\n",
    "print('Compressed pickle sanit size:', statinfo.st_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
